{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from libs.ds_charts import get_variable_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped variables ['PED_LOCATION', 'CONTRIBUTING_FACTOR_2', 'CONTRIBUTING_FACTOR_1', 'PED_ACTION']\n",
      "(45669, 17)\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = pd.read_csv('dataset_1/NYC_collisions_tabular.csv', na_values='NaN')\n",
    "new_dataset_1 = dataset_1.copy()\n",
    "\n",
    "#FIND VARIABLES WITH MISSING VALUES\n",
    "mv = {}\n",
    "for var in new_dataset_1:\n",
    "    nr = new_dataset_1[var].isna().sum()\n",
    "    if nr > 0:\n",
    "        mv[var] = nr\n",
    "\n",
    "#DISCARD COLUMNS WITH MORE THEN 90% MISSING VALUES\n",
    "threshold = new_dataset_1.shape[0] * 0.85\n",
    "\n",
    "missings = [c for c in mv.keys() if mv[c]>threshold]\n",
    "new_dataset_1.drop(columns=missings, inplace=True)\n",
    "print('Dropped variables', missings)\n",
    "\n",
    "#DISCARD RECORDS WITH MAJORITY OF MISSING VALUES\n",
    "threshold = new_dataset_1.shape[1] * 0.50\n",
    "\n",
    "new_dataset_1.dropna(thresh=threshold, inplace=True)\n",
    "print(new_dataset_1.shape)\n",
    "\n",
    "#PERSON_AGE\n",
    "person_age = dataset_1['PERSON_AGE']\n",
    "mean_ages = int(person_age.mean())\n",
    "new_dataset_1['PERSON_AGE'].fillna(mean_ages,inplace=True)\n",
    "\n",
    "#SAFETY_EQUIPMENT\n",
    "new_dataset_1['SAFETY_EQUIPMENT'].fillna('Unknown',inplace=True)\n",
    "\n",
    "#EJECTION\n",
    "new_dataset_1['EJECTION'].fillna('Not Ejected',inplace=True)\n",
    "\n",
    "#VEHICLE_ID\n",
    "new_dataset_1['VEHICLE_ID'].dropna(inplace=True)\n",
    "\n",
    "#POSITION IN VEHICLE\n",
    "new_dataset_1['POSITION_IN_VEHICLE'].fillna('Unknown',inplace=True)\n",
    "new_dataset_1.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRASH_DATE                     0\n",
      "CRASH_TIME                     0\n",
      "PERSON_AGE                     0\n",
      "VEHICLE_ID                     0\n",
      "PERSON_ID                      0\n",
      "                              ..\n",
      "POSITION_IN_VEHICLE_Unknown    0\n",
      "PED_ROLE_Driver                0\n",
      "PED_ROLE_Passenger             0\n",
      "PERSON_INJURY_Injured          0\n",
      "PERSON_INJURY_Killed           0\n",
      "Length: 88, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file = 'nyc_collisions'\n",
    "filename = 'data/nyc_collisions.csv'\n",
    "symbolic_vars = ['BODILY_INJURY','SAFETY_EQUIPMENT','PERSON_SEX','PERSON_TYPE','EJECTION','COMPLAINT','EMOTIONAL_STATUS','POSITION_IN_VEHICLE','PED_ROLE','PERSON_INJURY']\n",
    "\n",
    "def dummify(df, vars_to_dummify):\n",
    "    other_vars = [c for c in df.columns if not c in vars_to_dummify]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=bool)\n",
    "    X = df[vars_to_dummify]\n",
    "    encoder.fit(X)\n",
    "    new_vars = encoder.get_feature_names_out(vars_to_dummify)\n",
    "    trans_X = encoder.transform(X)\n",
    "    dummy = pd.DataFrame(trans_X, columns=new_vars, index=X.index)\n",
    "    dummy = dummy.convert_dtypes(convert_boolean=True)\n",
    "\n",
    "    final_df = pd.concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "variables = get_variable_types(new_dataset_1)\n",
    "new_dataset_1 = dummify(new_dataset_1, symbolic_vars)\n",
    "new_dataset_1.to_csv(f'data/{file}_dummified.csv', index=False)\n",
    "nr = new_dataset_1.isna().sum()\n",
    "print(nr)\n",
    "#df.describe(include=[bool])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "\n",
    "'''\n",
    " we need to apply the transfomation and rejoin the data together in order\n",
    " to have a unique dataframe. They can only be applied to numerical data, \n",
    " without any missing value. In order to do that, we are splitting our dataframe into three \n",
    " dataframes, one for each data type: above, discarding date variables, since the majority of \n",
    " techniques are not able to deal with them.\n",
    "'''\n",
    "variable_types = get_variable_types(new_dataset_1)\n",
    "numeric_vars = variable_types['Numeric']\n",
    "symbolic_vars2 = variable_types['Symbolic']\n",
    "boolean_vars = variable_types['Binary']\n",
    "\n",
    "df_nr = new_dataset_1[numeric_vars]\n",
    "df_sb = new_dataset_1[symbolic_vars2]\n",
    "df_bool = new_dataset_1[boolean_vars]\n",
    "\n",
    "print(numeric_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lw/g1mvrwwx7tg2dl6mr796qjt80000gn/T/ipykernel_12637/2747393366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_nr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_nr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnumeric_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnorm_data_minmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sb\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdf_bool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnorm_data_minmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/{file}_scaled_minmax.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_data_minmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_nr)\n",
    "tmp = pd.DataFrame(transf.transform(df_nr), index=df.index, columns= numeric_vars)\n",
    "norm_data_minmax = pd.concat([tmp, df_sb,  df_bool], axis=1)\n",
    "norm_data_minmax.to_csv(f'data/{file}_scaled_minmax.csv', index=False)\n",
    "print(norm_data_minmax.describe())\n",
    "df = new_dataset_1\n",
    "# Now we can se the result of the transformed data with a single boxplot, again.\n",
    "\n",
    "fig, axs = plt.pyplot.subplots(1, 3, figsize=(20,10),squeeze=False)\n",
    "axs[0, 0].set_title('Original data')\n",
    "df.boxplot(ax=axs[0, 0])\n",
    "axs[0, 1].set_title('Z-score normalization')\n",
    "norm_data_zscore.boxplot(ax=axs[0, 1])\n",
    "axs[0, 2].set_title('MinMax normalization')\n",
    "norm_data_minmax.boxplot(ax=axs[0, 2])\n",
    "plt.pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745d310d6c0ae55e49257fa2b34a349fe5a18b8d8dfaac89e4989c686accf5f0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('envComputerScience': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

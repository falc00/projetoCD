{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from libs.ds_charts import get_variable_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped variables ['PED_LOCATION', 'CONTRIBUTING_FACTOR_2', 'CONTRIBUTING_FACTOR_1', 'PED_ACTION']\n",
      "(45669, 17)\n",
      "CRASH_DATE             0\n",
      "CRASH_TIME             0\n",
      "PERSON_AGE             0\n",
      "BODILY_INJURY          0\n",
      "SAFETY_EQUIPMENT       0\n",
      "PERSON_SEX             0\n",
      "PERSON_TYPE            0\n",
      "EJECTION               0\n",
      "COMPLAINT              0\n",
      "EMOTIONAL_STATUS       0\n",
      "VEHICLE_ID             0\n",
      "PERSON_ID              0\n",
      "POSITION_IN_VEHICLE    0\n",
      "PED_ROLE               0\n",
      "UNIQUE_ID              0\n",
      "COLLISION_ID           0\n",
      "PERSON_INJURY          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = pd.read_csv('dataset_1/NYC_collisions_tabular.csv', na_values='NaN')\n",
    "new_dataset_1 = dataset_1.copy()\n",
    "\n",
    "#FIND VARIABLES WITH MISSING VALUES\n",
    "mv = {}\n",
    "for var in new_dataset_1:\n",
    "    nr = new_dataset_1[var].isna().sum()\n",
    "    if nr > 0:\n",
    "        mv[var] = nr\n",
    "\n",
    "#DISCARD COLUMNS WITH MORE THEN 90% MISSING VALUES\n",
    "threshold = new_dataset_1.shape[0] * 0.85\n",
    "\n",
    "missings = [c for c in mv.keys() if mv[c]>threshold]\n",
    "new_dataset_1.drop(columns=missings, inplace=True)\n",
    "print('Dropped variables', missings)\n",
    "\n",
    "#DISCARD RECORDS WITH MAJORITY OF MISSING VALUES\n",
    "threshold = new_dataset_1.shape[1] * 0.50\n",
    "\n",
    "new_dataset_1.dropna(thresh=threshold, inplace=True)\n",
    "print(new_dataset_1.shape)\n",
    "\n",
    "#PERSON_AGE\n",
    "person_age = dataset_1['PERSON_AGE']\n",
    "mean_ages = int(person_age.mean())\n",
    "new_dataset_1['PERSON_AGE'].fillna(mean_ages,inplace=True)\n",
    "\n",
    "#SAFETY_EQUIPMENT\n",
    "new_dataset_1['SAFETY_EQUIPMENT'].fillna('Unknown',inplace=True)\n",
    "\n",
    "#EJECTION\n",
    "new_dataset_1['EJECTION'].fillna('Not Ejected',inplace=True)\n",
    "\n",
    "#VEHICLE_ID\n",
    "new_dataset_1['VEHICLE_ID'].dropna(inplace=True)\n",
    "\n",
    "#POSITION IN VEHICLE\n",
    "new_dataset_1['POSITION_IN_VEHICLE'].fillna('Unknown',inplace=True)\n",
    "new_dataset_1.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRASH_DATE                  0\n",
      "CRASH_TIME                  0\n",
      "PERSON_AGE                  0\n",
      "VEHICLE_ID               6571\n",
      "PERSON_ID                   0\n",
      "                         ... \n",
      "PED_ROLE_Other              0\n",
      "PED_ROLE_Passenger          0\n",
      "PED_ROLE_Pedestrian         0\n",
      "PERSON_INJURY_Injured       0\n",
      "PERSON_INJURY_Killed        0\n",
      "Length: 95, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file = 'nyc_collisions'\n",
    "filename = 'data/nyc_collisions.csv'\n",
    "symbolic_vars = ['BODILY_INJURY','SAFETY_EQUIPMENT','PERSON_SEX','PERSON_TYPE','EJECTION','COMPLAINT','EMOTIONAL_STATUS','POSITION_IN_VEHICLE','PED_ROLE','PERSON_INJURY']\n",
    "\n",
    "def dummify(df, vars_to_dummify):\n",
    "    other_vars = [c for c in df.columns if not c in vars_to_dummify]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=bool)\n",
    "    X = df[vars_to_dummify]\n",
    "    encoder.fit(X)\n",
    "    new_vars = encoder.get_feature_names_out(vars_to_dummify)\n",
    "    trans_X = encoder.transform(X)\n",
    "    dummy = pd.DataFrame(trans_X, columns=new_vars, index=X.index)\n",
    "    dummy = dummy.convert_dtypes(convert_boolean=True)\n",
    "\n",
    "    final_df = pd.concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "variables = get_variable_types(new_dataset_1)\n",
    "df = dummify(new_dataset_1, symbolic_vars)\n",
    "df.to_csv(f'data/{file}_dummified.csv', index=False)\n",
    "nr = df.isna().sum()\n",
    "print(nr)\n",
    "#df.describe(include=[bool])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745d310d6c0ae55e49257fa2b34a349fe5a18b8d8dfaac89e4989c686accf5f0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('envComputerScience': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

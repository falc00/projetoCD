{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import libs.ds_charts as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped variables []\n",
      "(161631, 32)\n"
     ]
    }
   ],
   "source": [
    "dataset_2 = pd.read_csv('dataset_2/air_quality_tabular.csv', na_values='NaN')\n",
    "new_dataset_2 = dataset_2.copy()\n",
    "\n",
    "#FIND VARIABLES WITH MISSING VALUES\n",
    "mv = {}\n",
    "for var in new_dataset_2:\n",
    "    nr = new_dataset_2[var].isna().sum()\n",
    "    if nr > 0:\n",
    "        mv[var] = nr\n",
    "\n",
    "#DISCARD COLUMNS WITH MORE THEN 90% MISSING VALUES\n",
    "threshold = new_dataset_2.shape[0] * 0.90\n",
    "\n",
    "missings = [c for c in mv.keys() if mv[c]>threshold]\n",
    "new_dataset_2.drop(columns=missings, inplace=True)\n",
    "print('Dropped variables', missings)\n",
    "\n",
    "#DISCARD RECORDS WITH MAJORITY OF MISSING VALUES\n",
    "threshold = new_dataset_2.shape[1] * 0.50\n",
    "\n",
    "new_dataset_2.dropna(thresh=threshold, inplace=True)\n",
    "print(new_dataset_2.shape)\n",
    "\n",
    "# numeric values\n",
    "for column in mv:\n",
    "    if column != \"Field_1\":\n",
    "        vars = new_dataset_2[column]\n",
    "        mean_vars = int(vars.mean())\n",
    "        new_dataset_2[column].fillna(mean_vars,inplace=True)\n",
    "        \n",
    "new_dataset_2.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'FID', 'GbCity', 'GbProv', 'Field_1', 'ALARM', 'CO_Mean',\n",
      "       'CO_Min', 'CO_Max', 'CO_Std',\n",
      "       ...\n",
      "       'Prov_EN_Shaanxi', 'Prov_EN_Shandong', 'Prov_EN_Shanghai',\n",
      "       'Prov_EN_Shanxi', 'Prov_EN_Sichuan', 'Prov_EN_Tianjin',\n",
      "       'Prov_EN_Xinjiang', 'Prov_EN_Xizang', 'Prov_EN_Yunnan',\n",
      "       'Prov_EN_Zhejiang'],\n",
      "      dtype='object', length=385)\n"
     ]
    }
   ],
   "source": [
    "file = 'air_quality'\n",
    "filename = 'data/air_quality.csv'\n",
    "symbolic_vars = ['City_EN', 'Prov_EN']\n",
    "\n",
    "def dummify(df, vars_to_dummify):\n",
    "    other_vars = [c for c in df.columns if not c in vars_to_dummify]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=bool)\n",
    "    X = df[vars_to_dummify]\n",
    "    encoder.fit(X)\n",
    "    new_vars = encoder.get_feature_names(vars_to_dummify)\n",
    "    trans_X = encoder.transform(X)\n",
    "    dummy = pd.DataFrame(trans_X, columns=new_vars, index=X.index)\n",
    "    dummy = dummy.convert_dtypes(convert_boolean=True)\n",
    "\n",
    "    final_df = pd.concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "variables = ds.get_variable_types(new_dataset_2)\n",
    "new_dataset_2 = dummify(new_dataset_2, symbolic_vars)\n",
    "new_dataset_2.to_csv(f'data/{file}_dummified.csv', index=False)\n",
    "\n",
    "print(new_dataset_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.CO_Mean<10]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.CO_Max<40]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.CO_Std<15]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.NO2_Max<300]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.NO2_Std<60]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.O3_Max<400]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.O3_Std<125]\n",
    "new_dataset_2 = new_dataset_2[new_dataset_2.P<10]\n",
    "'''\n",
    "## WHERE IS PM2.5?\n",
    "\n",
    "numeric_vars = ['CO_Mean','CO_Min','CO_Max','CO_Std','NO2_Mean', 'NO2_Min', 'NO2_Max','NO2_Std', 'O3_Mean','O3_Min'\n",
    "                              ,'O3_Max', 'O3_Std', 'PM2.5_Mean', 'PM2.5_Min', 'PM2.5_Max', 'PM2.5_Std', 'PM10_Mean',\n",
    "                              'PM10_Min', 'PM10_Max', 'PM10_Std', 'SO2_Mean', 'SO2_Min', 'SO2_Max', 'SO2_Std']\n",
    "for num_var in new_dataset_2[numeric_vars]:\n",
    "    mean, std = np.mean(new_dataset_2[var]), np.std(new_dataset_2[var])\n",
    "    cut_off = std * 3\n",
    "    lower, upper = mean - cut_off, mean + cut_off\n",
    "    #outliers = [new_dataset_2.index[new_dataset_2[var] == x].tolist() for x in new_dataset_2[var] if x < lower or x > upper]\n",
    "    new_dataset_2 = new_dataset_2[(new_dataset_2[num_var] > lower) & (new_dataset_2[num_var] < upper)]\n",
    "    #for outlier in outliers:\n",
    "     #   new_dataset_2.drop(outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_types = ds.get_variable_types(new_dataset_2)\n",
    "numeric_vars = variable_types['Numeric']\n",
    "symbolic_vars = variable_types['Symbolic']\n",
    "boolean_vars = variable_types['Binary']\n",
    "\n",
    "df_nr = new_dataset_2[numeric_vars]\n",
    "df_sb = new_dataset_2[symbolic_vars]\n",
    "df_bool = new_dataset_2[boolean_vars]\n",
    "\n",
    "print(numeric_vars)\n",
    "print(symbolic_vars)\n",
    "print(boolean_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import DataFrame, concat\n",
    "\n",
    "transf = StandardScaler(with_mean=True, with_std=True, copy=True).fit(df_nr)\n",
    "tmp = DataFrame(transf.transform(df_nr), index=new_dataset_2.index, columns= numeric_vars)\n",
    "norm_data_zscore = concat([tmp, df_sb,  df_bool], axis=1)\n",
    "norm_data_zscore.to_csv(f'data/{file}_scaled_zscore.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame, concat\n",
    "\n",
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_nr)\n",
    "tmp = DataFrame(transf.transform(df_nr), index=new_dataset_2.index, columns= numeric_vars)\n",
    "norm_data_minmax = concat([tmp, df_sb,  df_bool], axis=1)\n",
    "norm_data_minmax.to_csv(f'data/{file}_scaled_minmax.csv', index=False)\n",
    "print(norm_data_minmax.describe())\n",
    "\n",
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "fig, axs = subplots(1, 3, figsize=(20,10),squeeze=False)\n",
    "axs[0, 0].set_title('Original data')\n",
    "new_dataset_2.boxplot(ax=axs[0, 0])\n",
    "axs[0, 1].set_title('Z-score normalization')\n",
    "norm_data_zscore.boxplot(ax=axs[0, 1])\n",
    "axs[0, 2].set_title('MinMax normalization')\n",
    "norm_data_minmax.boxplot(ax=axs[0, 2])\n",
    "show()\n",
    "\n",
    "norm_data_zscore.to_csv('data/air_quality_scaled_zscore.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TEST ZSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv, concat, unique, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_tag = 'air_quality_scaled_zscore'\n",
    "data: DataFrame = read_csv('data/air_quality_scaled_zscore.csv')\n",
    "target = 'ALARM'\n",
    "positive = 'Safe'\n",
    "negative = 'Danger'\n",
    "values = {'Original': [len(data[data[target] == positive]), len(data[data[target] == negative])]}\n",
    "\n",
    "y: np.ndarray = data.pop(target).values\n",
    "X: np.ndarray = data.values\n",
    "labels: np.ndarray = unique(y)\n",
    "labels.sort()\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "\n",
    "train = concat([DataFrame(trnX, columns=data.columns), DataFrame(trnY,columns=[target])], axis=1)\n",
    "train.to_csv(f'data/{file_tag}_train.csv', index=False)\n",
    "\n",
    "test = concat([DataFrame(tstX, columns=data.columns), DataFrame(tstY,columns=[target])], axis=1)\n",
    "test.to_csv(f'data/{file_tag}_test.csv', index=False)\n",
    "values['Train'] = [len(np.delete(trnY, np.argwhere(trnY==negative))), len(np.delete(trnY, np.argwhere(trnY==positive)))]\n",
    "values['Test'] = [len(np.delete(tstY, np.argwhere(tstY==negative))), len(np.delete(tstY, np.argwhere(tstY==positive)))]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "ds.multiple_bar_chart([positive, negative], values, title='Data distribution per dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "file = 'air_quality'\n",
    "filename = 'data/air_quality.csv'\n",
    "symbolic_vars = ['City_EN', 'Prov_EN']\n",
    "'''\n",
    "target = 'ALARM' #ou class?\\\n",
    "filename = 'air_quality_scaled_zscore'\n",
    "# versao ir buscar dados normalizados\n",
    "train: DataFrame = pd.read_csv(f'data/{filename}_train.csv')\n",
    "train.drop(['date','FID'],axis=1,inplace=True)\n",
    "trnY: ndarray = train.pop(target).values\n",
    "trnX: ndarray = train.values\n",
    "labels = pd.unique(trnY)\n",
    "labels.sort()\n",
    "\n",
    "test: DataFrame = pd.read_csv(f'data/{filename}_test.csv')\n",
    "test.drop(['date','FID'],axis=1,inplace=True)\n",
    "tstY: ndarray = test.pop(target).values\n",
    "tstX: ndarray = test.values\n",
    "\n",
    "nvalues = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
    "dist = ['manhattan', 'euclidean', 'chebyshev']\n",
    "values = {}\n",
    "best = (0, '')\n",
    "last_best = 0\n",
    "for d in dist:\n",
    "    yvalues = []\n",
    "    for n in nvalues:\n",
    "        knn = KNeighborsClassifier(n_neighbors=n, metric=d)\n",
    "        knn.fit(trnX, trnY)\n",
    "        prdY = knn.predict(tstX)\n",
    "        yvalues.append(accuracy_score(tstY, prdY))\n",
    "        if yvalues[-1] > last_best:\n",
    "            best = (n, d)\n",
    "            last_best = yvalues[-1]\n",
    "    values[d] = yvalues\n",
    "\n",
    "plt.figure()\n",
    "ds.multiple_line_chart(nvalues, values, title='KNN variants', xlabel='n', ylabel='accuracy', percentage=True)\n",
    "plt.savefig('images/{file_tag}_knn_study.png')\n",
    "show()\n",
    "print('Best results with %d neighbors and %s'%(best[0], best[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = knn = KNeighborsClassifier(n_neighbors=best[0], metric=best[1])\n",
    "clf.fit(trnX, trnY)\n",
    "prd_trn = clf.predict(trnX)\n",
    "prd_tst = clf.predict(tstX)\n",
    "ds.plot_evaluation_results(labels, trnY, prd_trn, tstY, prd_tst)\n",
    "plt.savefig('images/{file_tag}_knn_best.png')\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745d310d6c0ae55e49257fa2b34a349fe5a18b8d8dfaac89e4989c686accf5f0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('envComputerScience': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

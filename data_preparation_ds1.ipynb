{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config_prof'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, MinMaxScaler, StandardScaler\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mds_charts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_variable_types\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mds_charts\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mds\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projetoCD/libs/ds_charts.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simplefilter\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, plot_roc_curve\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfig_prof\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcfg\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_graphviz\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config_prof'"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from libs.ds_charts import get_variable_types\n",
    "import libs.ds_charts as ds\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv('data/NYC_collisions_tabular.csv', na_values='NaN')\n",
    "new_dataset_1 = dataset_1.copy()\n",
    "\n",
    "#FIND VARIABLES WITH MISSING VALUES\n",
    "mv = {}\n",
    "for var in new_dataset_1:\n",
    "    nr = new_dataset_1[var].isna().sum()\n",
    "    if nr > 0:\n",
    "        mv[var] = nr\n",
    "\n",
    "#DISCARD COLUMNS WITH MORE THEN 90% MISSING VALUES\n",
    "threshold = new_dataset_1.shape[0] * 0.85\n",
    "\n",
    "missings = [c for c in mv.keys() if mv[c]>threshold]\n",
    "new_dataset_1.drop(columns=missings, inplace=True)\n",
    "print('Dropped variables', missings)\n",
    "\n",
    "#DISCARD RECORDS WITH MAJORITY OF MISSING VALUES\n",
    "threshold = new_dataset_1.shape[1] * 0.50\n",
    "\n",
    "new_dataset_1.dropna(thresh=threshold, inplace=True)\n",
    "print(new_dataset_1.shape)\n",
    "\n",
    "#PERSON_AGE\n",
    "person_age = dataset_1['PERSON_AGE']\n",
    "mean_ages = int(person_age.mean())\n",
    "new_dataset_1['PERSON_AGE'].fillna(mean_ages,inplace=True)\n",
    "\n",
    "#SAFETY_EQUIPMENT\n",
    "new_dataset_1['SAFETY_EQUIPMENT'].fillna('Unknown',inplace=True)\n",
    "\n",
    "#EJECTION\n",
    "new_dataset_1['EJECTION'].fillna('Not Ejected',inplace=True)\n",
    "\n",
    "#VEHICLE_ID\n",
    "new_dataset_1['VEHICLE_ID'].dropna(inplace=True)\n",
    "\n",
    "#POSITION IN VEHICLE\n",
    "new_dataset_1['POSITION_IN_VEHICLE'].fillna('Unknown',inplace=True)\n",
    "new_dataset_1.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'nyc_collisions'\n",
    "filename = 'data/nyc_collisions.csv'\n",
    "week_days=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "### weekdays\n",
    "new_dataset_1['CRASH_DATE'] = new_dataset_1['CRASH_DATE'].apply(lambda x: week_days[datetime.date(int(x.split('/')[0]),int(x.split('/')[1]),int(x.split('/')[0])).weekday()])  \n",
    "\n",
    "symbolic_vars = ['CRASH_TIME','CRASH_DATE','BODILY_INJURY','SAFETY_EQUIPMENT','PERSON_SEX','EJECTION','PERSON_TYPE','COMPLAINT','EMOTIONAL_STATUS','POSITION_IN_VEHICLE','PED_ROLE']\n",
    "\n",
    "## Transform hours to 0-22\n",
    "new_dataset_1['CRASH_TIME'] = new_dataset_1['CRASH_TIME'].apply(lambda x: int(x.split(':')[0]))\n",
    "\n",
    "print(new_dataset_1.columns)\n",
    "\n",
    "def dummify(df, vars_to_dummify):\n",
    "    other_vars = [c for c in df.columns if not c in vars_to_dummify]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=bool)\n",
    "    X = df[vars_to_dummify]\n",
    "    encoder.fit(X)\n",
    "    new_vars = encoder.get_feature_names_out(vars_to_dummify)\n",
    "    trans_X = encoder.transform(X)\n",
    "    dummy = pd.DataFrame(trans_X, columns=new_vars, index=X.index)\n",
    "    dummy = dummy.convert_dtypes(convert_boolean=True)\n",
    "\n",
    "    final_df = pd.concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "variables = get_variable_types(new_dataset_1)\n",
    "new_dataset_1 = dummify(new_dataset_1, symbolic_vars)\n",
    "\n",
    "#remove ids, \n",
    "new_dataset_1.drop('PERSON_ID',axis=1,inplace=True)\n",
    "new_dataset_1.drop('UNIQUE_ID',axis=1,inplace=True)\n",
    "new_dataset_1.drop('COLLISION_ID',axis=1,inplace=True)\n",
    "new_dataset_1.drop('VEHICLE_ID',axis=1,inplace=True)\n",
    "new_dataset_1.drop('BODILY_INJURY_Unknown',axis=1,inplace=True)\n",
    "new_dataset_1.drop('SAFETY_EQUIPMENT_None',axis=1,inplace=True)\n",
    "new_dataset_1.drop('SAFETY_EQUIPMENT_Unknown',axis=1,inplace=True)\n",
    "new_dataset_1.drop('COMPLAINT_Unknown',axis=1,inplace=True)\n",
    "new_dataset_1.drop('EMOTIONAL_STATUS_Unknown',axis=1,inplace=True)\n",
    "new_dataset_1.drop('POSITION_IN_VEHICLE_Unknown',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "new_dataset_1.to_csv(f'data/{file}_dummified.csv', index=False)\n",
    "nr = new_dataset_1.isna().sum()\n",
    "\n",
    "print(new_dataset_1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE\n",
    "new_dataset_1 = new_dataset_1[(new_dataset_1.PERSON_AGE < 110) & (new_dataset_1.PERSON_AGE > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "'''\n",
    " we need to apply the transfomation and rejoin the data together in order\n",
    " to have a unique dataframe. They can only be applied to numerical data, \n",
    " without any missing value. In order to do that, we are splitting our dataframe into three \n",
    " dataframes, one for each data type: above, discarding date variables, since the majority of \n",
    " techniques are not able to deal with them.\n",
    "'''\n",
    "\n",
    "variable_types = get_variable_types(new_dataset_1)\n",
    "numeric_vars =  variable_types['Numeric'] # ['PERSON_AGE'] \n",
    "symbolic_vars2 = variable_types['Symbolic'] # ['CRASH_DATE, CRASH_TIME]\n",
    "boolean_vars = variable_types['Binary'] # all the symbolic are binary now!\n",
    "\n",
    "df_nr = new_dataset_1[numeric_vars]\n",
    "df_sb = new_dataset_1[symbolic_vars2]\n",
    "df_bool = new_dataset_1[boolean_vars]\n",
    "\n",
    "#print(numeric_vars)\n",
    "#print(symbolic_vars2)\n",
    "#print(boolean_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = StandardScaler(with_mean=True, with_std=True, copy=True).fit(df_nr)\n",
    "tmp = pd.DataFrame(transf.transform(df_nr), index=new_dataset_1.index, columns= numeric_vars)\n",
    "norm_data_zscore = pd.concat([tmp, df_sb,  df_bool], axis=1)\n",
    "norm_data_zscore.to_csv(f'data/{file}_scaled_zscore.csv', index=False)\n",
    "\n",
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_nr)\n",
    "tmp = pd.DataFrame(transf.transform(df_nr), index=new_dataset_1.index, columns= numeric_vars)\n",
    "norm_data_minmax = pd.concat([tmp, df_sb,  df_bool], axis=1)\n",
    "norm_data_minmax.to_csv(f'data/{file}_scaled_minmax.csv', index=False)\n",
    "print(norm_data_minmax.describe())\n",
    "df = new_dataset_1\n",
    "# Now we can se the result of the transformed data with a single boxplot, again.\n",
    "\n",
    "fig, axs = subplots(1, 3, figsize=(20,10),squeeze=False)\n",
    "axs[0, 0].set_title('Original data')\n",
    "df.boxplot(ax=axs[0, 0])\n",
    "axs[0, 1].set_title('Z-score normalization')\n",
    "norm_data_zscore.boxplot(ax=axs[0, 1])\n",
    "axs[0, 2].set_title('MinMax normalization')\n",
    "norm_data_minmax.boxplot(ax=axs[0, 2])\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN AND TEST ZSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv, concat, unique, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_tag = 'nyc_collisions_scaled_zscore'\n",
    "data: DataFrame = read_csv('data/nyc_collisions_scaled_zscore.csv')\n",
    "target = 'PERSON_INJURY'\n",
    "positive = 'Injured'\n",
    "negative = 'Killed'\n",
    "values = {'Original': [len(data[data[target] == positive]), len(data[data[target] == negative])]}\n",
    "\n",
    "y: np.ndarray = data.pop(target).values\n",
    "X: np.ndarray = data.values\n",
    "labels: np.ndarray = unique(y)\n",
    "labels.sort()\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "\n",
    "train = concat([DataFrame(trnX, columns=data.columns), DataFrame(trnY,columns=[target])], axis=1)\n",
    "train.to_csv(f'data/{file_tag}_train.csv', index=False)\n",
    "\n",
    "test = concat([DataFrame(tstX, columns=data.columns), DataFrame(tstY,columns=[target])], axis=1)\n",
    "test.to_csv(f'data/{file_tag}_test.csv', index=False)\n",
    "values['Train'] = [len(np.delete(trnY, np.argwhere(trnY==negative))), len(np.delete(trnY, np.argwhere(trnY==positive)))]\n",
    "values['Test'] = [len(np.delete(tstY, np.argwhere(tstY==negative))), len(np.delete(tstY, np.argwhere(tstY==positive)))]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "ds.multiple_bar_chart([positive, negative], values, title='Data distribution per dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN AND TEST MINMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv, concat, unique, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_tag = 'nyc_collisions_scaled_minmax'\n",
    "data: DataFrame = read_csv('data/nyc_collisions_scaled_minmax.csv')\n",
    "target = 'PERSON_INJURY'\n",
    "positive = 'Injured'\n",
    "negative = 'Killed'\n",
    "values = {'Original': [len(data[data[target] == positive]), len(data[data[target] == negative])]}\n",
    "\n",
    "y: np.ndarray = data.pop(target).values\n",
    "X: np.ndarray = data.values\n",
    "labels: np.ndarray = unique(y)\n",
    "labels.sort()\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "\n",
    "train = concat([DataFrame(trnX, columns=data.columns), DataFrame(trnY,columns=[target])], axis=1)\n",
    "train.to_csv(f'data/{file_tag}_train.csv', index=False)\n",
    "\n",
    "test = concat([DataFrame(tstX, columns=data.columns), DataFrame(tstY,columns=[target])], axis=1)\n",
    "test.to_csv(f'data/{file_tag}_test.csv', index=False)\n",
    "values['Train'] = [len(np.delete(trnY, np.argwhere(trnY==negative))), len(np.delete(trnY, np.argwhere(trnY==positive)))]\n",
    "values['Test'] = [len(np.delete(tstY, np.argwhere(tstY==negative))), len(np.delete(tstY, np.argwhere(tstY==positive)))]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "ds.multiple_bar_chart([positive, negative], values, title='Data distribution per dataset')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745d310d6c0ae55e49257fa2b34a349fe5a18b8d8dfaac89e4989c686accf5f0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('envComputerScience': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
